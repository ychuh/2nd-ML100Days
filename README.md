- Project: [æ©Ÿå™¨å­¸ç¿’ ç™¾æ—¥é¦¬æ‹‰æ¾](https://ai100-2.cupoy.com/participator/19013267/questions)   
- Duration: April 2019 / August 2019   
- Progress: **59 / 100**   

# [Reference]
- [Customizing Plots with Python Matplotlib](https://towardsdatascience.com/customizing-plots-with-python-matplotlib-bcf02691931f)
- [Linear Regression Simplified - Ordinary Least Square vs Gradient Descent](https://towardsdatascience.com/linear-regression-simplified-ordinary-least-square-vs-gradient-descent-48145de2cf76)

# [Preprocessing]

**[Day_1] Basic Calculating and Plot - 4/19**
- Mean Square Error: ğ‘€ğ‘†ğ¸=1ğ‘›âˆ‘ğ‘›ğ‘–=1(ğ‘Œğ‘–âˆ’ğ‘ŒÌ‚ğ‘–)2
- plt.plot(data_x, data_y, 'b-', label = 'label_name')

**[Day_2] EDA-1/è®€å–è³‡æ–™ - 4/20**
- Evaluation of classification: AUC, ROC
- f_app = os.path.join(dir_data, 'application_train.csv')
- DataFrame.shape/describe/head/tail
- DataFrame['Col1', 'Col2']: extract col1 and col2 only

**[Day_3] Build DataFrame and Get/Extract Figure from URL- 4/21**
- df.loc[condition, tartget_columns]
- np.random.randint(range, number)
- requests.get(URL)
- result = img2arr_fromURLs(df.loc[0:4]['URL']): get images
- plt.imshow()/ plt.show()

**[Day_4] Concept of Sub DataFrame, LabelEncoder, OneHotEncoder - 4/23**
- sub_df = pd.DataFrame(df['col1', 'col2'])

**[Day_5] Calculate Mean, Max, Min of Data and Plot Histogram - 4/24**
- df.mean()/df.max()/df.min()
- plt.hist()

**[Day_6] Detect Outliers - 4/29**
- df.boxplot()
- Emprical Cumulative Density Plot (ECDP)
- df.loc

[Day_7]

[Day_8]

[Day_9]

[Day_10]

[Day_11]

[Day_12]

[Day_13]

**[Day_14] Learn Subplot Method - 5/5**
- plt.supplot(#row, #col, location)

**[Day_15] Concept of Heatmap, PairPlot - 5/5**
- plt.figure(figsize = (width, height))
- sns.heatmap()
- np.random.sample([nrow, ncol]): create a random value matrix
- sns.PairGrid()

**[Day_16] First Model - 5/5**
- LogisticRegression()
- submit.to_csv(filename, index)

## [Feature Engineering]

**[Day_17] Introducing Feature Engineering - 5/7**
- LabelEncoder(): labeling object data
- MinMaxScaler()

**[Day_18] Distinquish Data Type and Split It Into Different Groups - 5/10**
- Int, Float, Object etc.
- zip(df.dtypes, df.columns)

**[Day_19] Dealing With Missing Data - 5/10**
- DataFrame.fillna(): fill missing data with 0, -1, mean, median to make dataset meaningful
- MinMaxScaler(), StandardScaler()
- cross_val_score(estimator, training set, target, cross_validation_times)

**[Day_20] Dealing With Outliers - 5/10**
- DataFrame.clip(): set a upper/lower limits and transform outliers into upper/lower limit value
- Drop outlier directly

**[Day_21] Reduce Skewness (ååº¦) - 5/11**
- Stats.boxcox()

**[Day_22] Impact on LogisticRegression with LabelEncoder/OneHotEncoder - 5/11**
- LabelEncoder(), pd.get_dummies()

**[Day_23] Dealing With Oject Data by Mean Encoder - 5/12**
- Smoothing
- pd.concat(df_1, df_2, axis=1): axis=1 align by column
- data = pd.concat([df[:train_num], train_Y], axis=1): initiate for mean encoding, adding target column back
- mean_df = data.groupby([c])['Survived'].mean().reset_index()
- mean_df.columns = [c, f'{c}_mean']
- data = pd.merge(data, mean_df, on=c, how='left')
- data = data.drop([c] , axis=1)

**[Day_24] CountEncoder And FeatureHash - 5/14**

**[Day_25] Time Feature - 5/15**
- datetime.weekday()
- datetime.isoweekday()
- datetime.isocalendar()
- DataFrame.apply(lambda x: x.weekday()) **v.s.** DataFrame.map(lambda x: math.sin(x*math.pi))

**[Day_26] - 5/16**

**[Day_27] Groupby Encoder - 5/18**
- å‰µç«‹å…©ç¨®ä»¥ä¸Šçš„ç¾¤èšç·¨ç¢¼ç‰¹å¾µ(meanã€medianã€modeã€maxã€minã€count)
- Feature å¯§çˆ›å‹¿ç¼ºã€å¤šå¤šç›Šå–„

**[Day_28] Feature Selection: Filter, Wrapper, Embedded - 5/18**
- éæ¿¾æ³• (Filter) : é¸å®šçµ±è¨ˆæ•¸å€¼èˆ‡è¨­å®šâ¾¨é–€æª»ï¼Œåˆªé™¤ä½æ–¼â¾¨é–€æª»çš„ç‰¹å¾µ
- åŒ…è£æ³• (Wrapper) : æ ¹æ“šâ½¬ç›®æ¨™å‡½æ•¸ï¼Œé€æ­¥åŠ å…¥ç‰¹å¾µæˆ–åˆªé™¤ç‰¹å¾µ
- åµŒå…¥æ³• (Embedded) : ä½¿â½¤ç”¨æ©Ÿå™¨å­¸ç¿’æ¨¡å‹ï¼Œæ ¹æ“šæ“¬åˆå¾Œçš„ä¿‚æ•¸ï¼Œåˆªé™¤ä¿‚æ•¸ä½æ–¼â¾¨é–€æª»çš„ç‰¹å¾µ

**[Day_29] Feature Importance - 5/19**
- estimator = GradientBoostingClassifier()
- estimator.fit(df.values, train_Y)
- feats = pd.Series(data=estimator.feature_importances_, index=df.columns)
- feats = feats.sort_values(ascending=False)

**[Day_30] Leaf Encodeing - 5/25**
***
## [Machine Learning]

**[Day_31] Concenpt of Machine Learning - 5/25**

**[Day_32] Process of Machine Learning - 5/28**
- Project of Personalized Hey Siri (Speaker Recognition, DNN, RNN)

**[Day_33] How Does A Machine Learn? -5/28**
- Over-fitting, éæ“¬åˆä»£è¡¨æ¨¡å‹å¯èƒ½å­¸ç¿’åˆ°è³‡æ–™ä¸­çš„å™ªâ¾³éŸ³ï¼Œå°è‡´åœ¨å¯¦éš›æ‡‰â½¤ç”¨æ™‚é æ¸¬å¤±æº–

**[Day_34] Train_Test_Split and K-Fold - 5/30**
- Train_Test_Split(test_size)

**[Day_35]

**[Day_36] Evaluation Metrics - 6/2**
- ROC
- AUC
- Precision-Recall
- TP, TN, FP, FN
- F_beta_score
- Confusion Metrics

**[Day_37] Regression Model Intro - 6/2**
- Linear Regression (to find a linear equation which represents the relationship of inputs and outputs)
- Logistic Regression (a classifcation model)

**[Day_38] Regression Model - 6/2**
- Linear Regression, Logistic Regression, Multinominal Logistic Regression

**[Day_39] Lasso, Ridge Regression Intro- 6/2**
- Loss function, æå¤±å‡½æ•¸è¡¡é‡ï¥¾é æ¸¬å€¼èˆ‡å¯¦éš›å€¼çš„å·®ç•°ï¥¢ï¼Œè®“æ¨¡å‹èƒ½å¾€æ­£ç¢ºçš„â½…æ–¹å‘å­¸ç¿’ (e.g. MSE, MAE, RMSE...)
- Regulization, å‰‡æ˜¯é¿å…æ¨¡å‹è®Šå¾—éæ–¼è¤‡é›œï¼Œé€ æˆéæ“¬åˆ (Over-fitting) (e.g. L1, L2 é€™ï¥¸ç¨®éƒ½æ˜¯å¸Œæœ›æ¨¡å‹çš„åƒæ•¸æ•¸å€¼ä¸è¦å¤ªâ¼¤ï¼Œé™ä½å°å™ªéŸ³æ•æ„Ÿåº¦ï¼Œ**æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½â¼’**)
- Lasso = linear regression + **L1**, can be used to do feature selection
- Ridge = linear regression + **L2**, can use to solve multicolinearity

**[Day_40] Lasso, Ridge Regression - 6/2**

**[Day_41] Introduction of Decision Tree - 6/3**

**[Day_42] Coding of Decision Tree - 6/4**
- DecisionTreeClassifier()
- DecisionTreeRegressor()

**[Day_43] Introduction of Random Forest - 6/5**
- Can be used as Classification and Regression Tree
- Can reduce probability of overfitting by split features into random number of trees, nodes
- Bagging -> Bulid Tree -> Ensemble (Classifier: voting; Regression: mean value)

**[Day_44] Practice of Random Forest - 6/6**
- n_estimator, how many trees created
- max_depth

**[Day_45] GradientBoostingMachine**

> _More complex model will be more accurate (__lower bias__), but will be overfitting (__higher variance__)_

> _Recommanded max_depth of gbdt is **8**; and RandomForestTree is **15**_

![Model Complexity and Error](https://img.ifun01.com/images/2016/09/24/122036_XCWmIB.png!r800x0.jpg)

- xgboost: æœ€å¤§çš„ç‰¹é»åœ¨äºï¼Œå®ƒèƒ½å¤ è‡ªå‹•åˆ©ç”¨CPUçš„å¤šç·šç¨‹é€²è¡Œä¸¦è¡Œï¼ŒåŒæ™‚åœ¨ç®—æ³•ä¸ŠåŠ ä»¥æ”¹é€²æé«˜äº†ç²¾åº¦ã€‚
- [ä¸ºä»€ä¹ˆè¯´baggingæ˜¯å‡å°‘varianceï¼Œè€Œboostingæ˜¯å‡å°‘bias?](https://www.zhihu.com/question/26760839)
- [ä¸ºä»€ä¹ˆxgboost/gbdtåœ¨è°ƒå‚æ—¶ä¸ºä»€ä¹ˆæ ‘çš„æ·±åº¦å¾ˆå°‘å°±èƒ½è¾¾åˆ°å¾ˆé«˜çš„ç²¾åº¦ï¼Ÿ](https://www.zhihu.com/question/45487317)

**[Day_50] Stacking - 6/25**
- [å¦‚ä½•åœ¨ Kaggle é¦–æˆ˜ä¸­è¿›å…¥å‰ 10%](https://dnc1994.com/2016/04/rank-10-percent-in-first-kaggle-competition/)

**[Day_54] Introduction of Unsupervised Learning - 6/26**

**[Day_55] Clustering - 6/27**
- Kmeans is to minimize the within-cluster sum of squares (WCSS)
- ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)

**[Day_56] Silhouette Analysis - 6/28**

**[Day_57] Hierarchical Clustering, Axes3D - 6/28**
- éšå±¤å¼åˆ†ç¾¤åœ¨ç„¡éœ€å®šç¾©ç¾¤æ•¸çš„æƒ…æ³ä¸‹åšè³‡æ–™çš„åˆ†ç¾¤ï¼Œâ½½è€Œå¾Œå¯ä»¥â½¤ç”¨ä¸åŒçš„è·é›¢å®šç¾©â½…æ–¹å¼æ±ºå®šè³‡æ–™ç¾¤çµ„ã€‚
- åˆ†ç¾¤è·é›¢è¨ˆç®—â½…æ–¹å¼æœ‰ single-link, complete-link, average-linkã€‚
- æ¦‚å¿µï¦£ç°¡å–®ä¸”å®¹æ˜“ï§ å‘ˆç¾ï¼Œä½†ä¸é©åˆâ½¤ç”¨åœ¨â¼¤å¤§è³‡æ–™ã€‚

**[Day_58] Hierarchical Clustering in 2D - 6/29**

**[Day_59] Dimension Reduction_1, PCA, Principal Component Analysis - 6/30**
- Eigen value, eigenvector
  - å¦‚æœå­˜åœ¨ä¸€å€‹éé›¶çš„å‘é‡ xï¼Œä½¿å¾— x è¢« A ä½œç”¨ä¹‹å¾Œ (ä¹Ÿå°±æ˜¯ A*x)ï¼Œå…¶çµæœæœƒæ˜¯ x çš„ç°¡å–®å¸¸æ•¸å€ (Î»)ï¼Œä¹Ÿå°±æ˜¯ï¼šAx = Î»xï¼Œå‰‡ç¨± x ç‚º A çš„ç‰¹å¾µå‘é‡ï¼ŒÎ» ç‚º A çš„ç‰¹å¾µå€¼ã€‚
- [U, S, V] = svd(Sigma), singular value decomposition
- é™ä½ç¶­åº¦å¯ä»¥å¹«åŠ©æˆ‘å€‘å£“ç¸®åŠä¸Ÿæ£„ç„¡â½¤ç”¨è³‡è¨Šã€æŠ½è±¡åŒ–åŠçµ„åˆæ–°ç‰¹å¾µã€å‘ˆç¾â¾¼é«˜ç¶­æ•¸æ“šã€‚å¸¸â½¤ç”¨çš„ç®—æ³•çˆ²ä¸»æˆåˆ†åˆ†æã€‚
- åœ¨ç¶­åº¦å¤ªâ¼¤å¤§ç™¼â½£ç”Ÿ overfitting çš„æƒ…æ³ä¸‹ï¼Œå¯ä»¥å˜—è©¦â½¤ç”¨PCA çµ„æˆçš„ç‰¹å¾µä¾†ï¤­åšç›£ç£å¼å­¸ç¿’ï¼Œä½†ä¸å»ºè­°â¼€ä¸€é–‹å§‹å°±åšã€‚
